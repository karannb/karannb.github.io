<!DOCTYPE HTML>
<html lang="en">
<head>
    <!-- Hi, Karan Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FZYHPBJ2NN"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FZYHPBJ2NN');
    </script>

    <link rel="icon" type="image/png" href="images/candle.jpg">
    <base target="_blank">
</head>

<style>
    /* Custom Scrollbar */
    details summary {
        cursor: pointer;
        list-style: none;
    }
    details summary::-webkit-details-marker {
        display: none;
    }
    details summary:hover {
        text-decoration: underline;
    }
    .news-container {
        max-width: 800px;
    }
    
    .news-scroll {
        height: 200px;
        overflow-y: auto;
        background: #fff;
    }
    
    .news-scroll h3 {
        margin-top: 0;
        position: sticky;
        top: 0;
        background: #fff;
    }
    
    .news-list {
        list-style: none;
        padding: 0;
        margin: 0;
    }
    
    .news-item {
        padding: 12px 0;
    }
    
    .news-item:last-child {
        border-bottom: none;
    }

    .news-scroll {
        height: 200px;
        overflow-y: auto;
        background: #fff;
        scrollbar-width: none;  /* Firefox */
        -ms-overflow-style: none;  /* Internet Explorer/Edge */
    }

    .news-scroll::-webkit-scrollbar {
        display: none; /* Chrome, Safari, Opera */
    }

    /* Hyperlink modification */
    a {
        color: black;
        text-decoration: underline;
    }
    a:hover {
        color: gray;
        text-decoration: underline;
    }

    /* Hide the default triangle */
    details > summary {
        list-style: none;
        cursor: pointer;
        position: relative;
        padding-left: 15px;
        align-items: center;  /* Vertical alignment */
    }
    
    /* Remove the default marker in Firefox */
    details > summary::-webkit-details-marker,
    details > summary::marker {
        display: none;
    }
    
    /* Create custom + indicator */
    details > summary:before {
        content: '+';
        position: absolute;
        left: 0;
        color: gray;
        align-items: center;
        height: 100%;
    }
    
    /* Change to - when open */
    details[open] > summary:before {
        content: '-';
    }
</style>

<body>
    <!-- 
        Intro Stuff
    -->
    <div style="display: flex; align-items: flex-start; gap: 2rem;">
        <img src="images/portrait.jpg" alt="Karan Bania" width="300" height="398">
        <div style="width: 500px;">
            <div style="display: flex; align-items: center; gap: 8px;">
                <h2>Karan Bania</h2>
                <span>(‡™ï‡™∞‡™® ‡™¨‡™®‡™ø‡™Ø‡™æ)</span>
            </div>
            <p>
                Computer Science Undergraduate at <a href="https://www.bits-pilani.ac.in/goa/">BITS Goa, India</a>,
                Upcoming Research Engineer at <a href="https://gan.ai/">Gan.AI</a>,
                Prev-<a href="https://drive.google.com/file/d/1u3ic72_KQ-uYeB2laLx-nJvxbFdjBo6x/view?usp=sharing">DAAD WISE</a>'24 Scholar.
            </p>

            <p>
                E-Mail: echo "pfwfs3gfsnf3>Elrfnq3htr" | tr "\63-\172\056-\62" "\056-\172"
            </p>

            <p>
                ~ 
                <a href="https://drive.google.com/file/d/1AWvmQdRjp2H3Hbl0473NHN6BU2x_BEy8/view?usp=drive_link">CV</a> 
                / <a href="https://github.com/karannb">GitHub</a> 
                / <a href="https://scholar.google.com/citations?user=JOIQduEAAAAJ&hl=en">Scholar</a>
                ~
            </p>

            <!-- 
                About
            -->
            <p>
                I really like rap music, <a href="https://open.spotify.com/playlist/2VvPmWQzgJ3wtMUraEZ6UZ?si=b8251b952d674537">rap&blues</a>;
                avid Pok√©mon catcher ;), I have also gained a lot of interest in AI driven Materials Design and it's massive potential
                applications.
                <br>
                Sometimes, I also (try to) talk about research, here's the better ones, 
                <a href="https://docs.google.com/presentation/d/1UmnUMkBmlkAYAH1Brg5IgoUDFHZpdbyJLDUlBYsI_Ns/edit?usp=sharing">Graphormer</a>,
                <a href="https://docs.google.com/presentation/d/1sx4xFZU90V2TebRmvc31Dyyqt7_7m4mA2BEhC65iK6k/edit?usp=sharing">Radio-LM</a>,
                Deep SSMs [<a href="https://docs.google.com/presentation/d/14rBxmHsfTxxQ_L7wYW91_qlDyke078tja6dL32UaGnA/edit?usp=sharing">Slides</a> /
                <a href="https://youtu.be/cAfrHICmDk0">Video</a>].
            </p>
        </div>
    </div>
    <br>
    <br>

    <!-- 
        News
    -->
    <div class="news-scroll">
        <h3>News</h3>
        <ul style="text-align: left; max-width: 800px;">
            <li>
                <i>11/24</i> - My DAAD WISE project is on <a href="https://arxiv.org/abs/2411.12603">arXiv</a>.
            </li>
            <li>
                <i>10/24</i> - Check out our annual <a href = "https://sites.google.com/goa.bits-pilani.ac.in/ai-symposium-2024/home">AI Symposium</a>! 
                    Register <a href = "https://forms.gle/vMmt6UR6pc3boZS3A">here</a>. 
            </li>
            <li>
                <i>10/24</i> - Both of my APPCAIR projects' arXiv reports were released, <a href="https://arxiv.org/abs/2410.22862">AtGCN</a>
                & <a href="https://arxiv.org/abs/2410.20600">PXP</a>.
            </li>
            <li>
                <i>09/24</i> - Got a placement offer from <a href="https://gan.ai/">Gan.AI</a>.
            </li>
            <li>
                <i>08/24</i> - Made the President of <a href="https://www.saidl.in/home">SAiDL</a> for the year 2024-25.
            </li>
            <li>
                <i>08/24</i> - We released an <a href="https://arxiv.org/abs/2408.06261">arXiv</a> report for our work on MolGAN and Normalizing flows.
            </li>
            <li>
                <i>08/24</i> - TAing the CS F351 - Theory of Computation course at BITS Goa this semester. (<a href="https://drive.google.com/drive/folders/1liFwswHZn9otRC0VQcZ6vzTc4yBvqHYl?usp=drive_link">Tutorials</a>)
            </li>
            <li>
                <i>07/24</i> - I am a DataThon volunteer at <a href="https://indoml.in/indoml_datathon.php">IndoML'24</a>, The contest goes live on 5th August, register <a href="https://forms.gle/cVzA1cwyzuvXtzh17">here</a>! 
            </li>
            <li>
                <i>07/24</i> - A SAiDL blogpost led by me, on <a href="https://arxiv.org/abs/2106.05234">Graphormer</a> was accepted to the <a href="https://gram-workshop.github.io/index.html">GRaM Workshop at ICML'24</a>. 
                It is live <a href="https://gram-blogposts.github.io/blog/2024/graphormer/">here</a>.
            </li>
            <li>
                <i>06/24</i> - Our reproduction of <a href="https://arxiv.org/abs/2302.12066">Teaching CLIP to Count to Ten</a> is on <a href="https://arxiv.org/abs/2406.03586">arXiv</a>.
            </li>
            <li>
                <i>05/24</i> - Started my DAAD WISE Internship.
            </li>
            <li>
                <i>04/24</i> - Won first prize in ACM Goa Chapter's event PitchIt! organized at BITS Goa for my research presentation. 
                You can check out the slides <a href="https://docs.google.com/presentation/d/1cgqOCkLUlZULUnKfNyfb5xOiYnrGhVsb/edit?usp=sharing&ouid=103617805505848253890&rtpof=true&sd=true">here</a>.
            </li>
            <li>
                <i>02/24</i> - Our Spring Induction assignment for the 2024-25 cycle is out! 
                [<a href="https://github.com/SforAiDl/SAiDL-Spring-2024-Induction-Assignment">Link</a>]
                Fill out this <a href="https://forms.gle/WixhqzGdo3ZjUww98">form</a> and join the <a href="https://join.slack.com/t/saidl/shared_invite/zt-1al0wkfhz-HzI7hLkWkOhBpP1zaGGSVg">Slack</a> workspace for any doubts. Looking forward to your submissions!
            </li>
            <li>
                <i>02/24</i> - Selected for the <a href="https://www2.daad.de/deutschland/stipendium/datenbank/en/21148-scholarship-database/?detail=50015295">DAAD WISE program</a> 
                (one of 250 undergraduates from India) at RUB, Germany for summer 2024! 
                <a href="https://www.ini.rub.de/">Institut F√ºr Neuroinformatik</a> will be hosting me.
            </li>
            <li>
                <i>01/24</i> - Selected for the 
                <a href="https://www.mitacs.ca/our-programs/globalink-research-internship-students/">MITACS GRI program</a> 
                (over 30,000 applicants) at the Ontario Tech University, Canada for summer 2024! 
                <a href="http://vclab.science.ontariotechu.ca/">VC Lab</a> will be hosting me. (I rejected the offer)
            </li>
            <li>
                <i>01/24</i> - TAing the BITS F464 - Machine Learning course at BITS Goa this semester. (You can check out our labs 
                <a href="https://github.com/karannb/bits-f464-labs">here</a>)
            </li>
            <li>
                <i>01/24</i> - Made the General Secretary of <a href="https://www.saidl.in/home">SAiDL</a> for the year 2023-24.
            </li>
            <li>
                <i>12/23</i> - Started contributing to <a href="https://deepchem.io">DeepChem</a>.
            </li>
            <li>
                <i>10/23</i> - Check out our annual <a href = "https://sites.google.com/goa.bits-pilani.ac.in/ai-symposium-2023/home">AI Symposium</a>! 
                    Register <a href = "https://docs.google.com/forms/d/e/1FAIpQLSfmFMEGHAhfScJ1kZIyC8dUjYjUxStbR1H1eKL3IZqhWC7Xtw/viewform">here</a>. 
                    [<a href="https://linktr.ee/ai_symposium">linktree</a>]
            </li>
            <li>
                <i>08/23</i> - Started working at <a href="https://www.bits-pilani.ac.in/appcair/">APPCAIR</a> as a student researcher.
            </li>
            <li>
                <i>08/23</i> - TAing the CS F214 - Logic in Computer Science course at BITS Goa this semester.
            </li>
            <li>
                <i>07/23</i> - One of four people from our batch to get inducted into <a href="https://www.saidl.in">SAiDL</a>!
            </li>
            <li>
                <i>06/23</i> - Selected for <a href="http://cvit.iiit.ac.in/summerschool2023/">IIITH's 7th Summer School on AI</a> with focus on Computer Vision.
            </li>
            <li>
                <i>05/23</i> - Selected for <a href="https://academy.neuromatch.io/">Neuromatch Summer School</a> in the Deep Learning Track.
            </li>
        </ul>
    </div>
    <br>


    <!-- 
        Projects
    -->
    <h3>Projects & Publications</h3>
    <ul style="text-align: left">
        <!-- ESML -->
        <li>
            <p>
                <b>STREAM: A Universal State-Space Model for Sparse Geometric Data</b>
                [<a href="https://arxiv.org/abs/2411.12603">arXiv</a>]
                <img src="images/rub-logo.png" width="56" height="36">
                <img src="images/daad-logo.png" width="36" height="36">
                <br>
                Mark Sch√∂ne*, Yash Bhisikar*, Karan Bania*, Khaleelulla Khan Nazeer, Christian Mayr, Anand Subramoney, David Kappel
            </p>
            <details>
                <summary style="color: gray;">abstract</summary>
                <p style="width: 800px;">
                Handling sparse and unstructured geometric data, such as point clouds or event-based vision, is a pressing challenge in the field of machine vision. 
                Recently, sequence models such as Transformers and state-space models entered the domain of geometric data. 
                These methods require specialized preprocessing to create a sequential view of a set of points. 
                Furthermore, prior works involving sequence models iterate geometric data with either uniform or learned step sizes, implicitly relying on the model to infer the underlying geometric structure. 
                In this work, we propose to encode geometric structure explicitly into the parameterization of a state-space model. 
                State-space models are based on linear dynamics governed by a one-dimensional variable such as time or a spatial coordinate. 
                We exploit this dynamic variable to inject relative differences of coordinates into the step size of the state-space model. 
                The resulting geometric operation computes interactions between all pairs of N points in O(N) steps. 
                Our model deploys the Mamba selective state-space model with a modified CUDA kernel to efficiently map sparse geometric data to modern hardware. 
                The resulting sequence model, which we call STREAM, achieves competitive results on a range of benchmarks from point-cloud classification to event-based vision and audio classification. 
                STREAM demonstrates a powerful inductive bias for sparse geometric data by improving the PointMamba baseline when trained from scratch on the ModelNet40 and ScanObjectNN point cloud analysis datasets. 
                It further achieves, for the first time, 100% test accuracy on all 11 classes of the DVS128 Gestures dataset.
                </p>
            </details>
        </li>
        <!-- ATAXIA-SYNC -->
        <li>
            <p>
                <b>AtGCN: A Graph Convolutional Network For Ataxic Gait Detection</b>
                [<a href="https://arxiv.org/abs/2410.22862">arXiv</a>]
                <img src="images/bits-logo.png" width="36" height="36">
                <br>
                Karan Bania, Tanmay Verlekar
            </p>
            <details>
                <summary style="color: gray;">abstract</summary>
                <p style="width: 800px;">
                Video-based gait analysis can be defined as the task of diagnosing pathologies, such as ataxia, using videos of patients walking in front of a camera. 
                This paper presents a graph convolution network called AtGCN for detecting ataxic gait and identifying its severity using 2D videos. 
                The problem is especially challenging as the deviation of an ataxic gait from a healthy gait is very subtle. 
                The datasets for ataxic gait detection are also quite small, with the largest dataset having only 149 videos. 
                The paper addresses the first problem using special spatiotemporal graph convolution that successfully captures important gait-related features. 
                To handle the small dataset size, a deep spatiotemporal graph convolution network pre-trained on an action recognition dataset is systematically truncated and then fine-tuned on the ataxia dataset to obtain the AtGCN model. 
                The paper also presents an augmentation strategy that segments a video sequence into multiple gait cycles. 
                The proposed AtGCN model then operates on a graph of body part locations belonging to a single gait cycle. 
                The evaluation results support the strength of the proposed AtGCN model, as it outperforms the state-of-the-art in detection and severity prediction with an accuracy of 93.46% and a MAE of 0.4169, respectively.
                </p>
            </details>
        </li>
        <!-- PXP -->
        <li>
            <p>
                <b>Implementation and Application of an Intelligibility Protocol for Interaction with an LLM</b>
                (PXP)
                [<a href="https://arxiv.org/abs/2410.20600">arXiv</a> / <a href="https://github.com/karannb/interact">GitHub</a>]
                <img src="images/macquarie-logo.png" width="30" height="36">
                <img src="images/appcair-logo.jpg" width="45" height="45">
                <img src="images/bits-logo.png" width="36" height="36">
                <br>
                Ashwin Srinivasan, Karan Bania, Shreyas V, Harshvardhan Mestha, Sidong Liu
            </p>
            <details>
                <summary style="color: gray; ;">abstract</summary>
                <p style="width: 800px;">
                Our interest is in constructing interactive systems involving a human-expert interacting with a machine learning engine on data analysis tasks. 
                This is of relevance when addressing complex problems arising in areas of science, the environment, medicine and so on, which are not immediately amenable to the usual methods of statistical or mathematical modelling. 
                In such situations, it is possible that harnessing human expertise and creativity to modern machine-learning capabilities of identifying patterns by constructing new internal representations of the data may provide some insight to possible solutions. 
                In this paper, we examine the implementation of an abstract protocol developed for interaction between agents, each capable of constructing predictions and explanations. 
                The PXP protocol, described in [12] is motivated by the notion of ''two-way intelligibility'' and is specified using a pair of communicating finite-state machines. 
                While the formalisation allows the authors to prove several properties about the protocol, no implementation was presented. 
                Here, we address this shortcoming for the case in which one of the agents acts as a ''generator'' using a large language model (LLM) and the other is an agent that acts as a ''tester'' using either a human-expert, or a proxy for a human-expert (for example, a database compiled using human-expertise). 
                We believe these use-cases will be a widely applicable form of interaction for problems of the kind mentioned above. 
                We present an algorithmic description of general-purpose implementation, and conduct preliminary experiments on its use in two different areas (radiology and drug-discovery). 
                The experimental results provide early evidence in support of the protocol's capability of capturing one- and two-way intelligibility in human-LLM in the manner proposed in [12].
                </p>
            </details>
            <br>
            This work's presentation led to the first prize in an ACM Goa event, <a href="https://docs.google.com/presentation/d/1cgqOCkLUlZULUnKfNyfb5xOiYnrGhVsb/edit?usp=sharing&ouid=103617805505848253890&rtpof=true&sd=true">slides</a>.
        </li>
        <!-- DFS -->
        <li>
            <p>
                <b>Deep Forest Sciences</b>
                <img src="images/dfs-logo.png" width="36" height="36">
                <img src="images/deepchem-logo.png" width="36" height="40">
                <img src="images/amat-logo.png" width="36" height="36">
            </p>
            <p>
                Previously, I co-developed MolGAN and Normalizing Flow implementations in PyTorch [<a href="https://arxiv.org/abs/2408.06261">arXiv</a>]. 
                <details>
                    <summary style="color: gray;">abstract</summary>
                    <p style="width: 800px;">
                    Generative models for molecules have shown considerable promise for use in computational chemistry, but remain difficult to use for non-experts. 
                    For this reason, we introduce open-source infrastructure for easily building generative molecular models into the widely used DeepChem library with the aim of creating a robust and reusable molecular generation pipeline. 
                    In particular, we add high quality PyTorch implementations of the Molecular Generative Adversarial Networks (MolGAN) and Normalizing Flows. 
                    Our implementations show strong performance comparable with past work.
                </details>
                <br>
                Contributing to the <a href="https://deepchem.io">DeepChem</a> library.
                I have added several tutorials (and adding more basic equivariance tutorials), 
                [<a href="https://deepchem.io/tutorials/introduction-to-material-science/">1</a>]
                [<a href="https://deepchem.io/tutorials/generating-molecules-with-molgan/">2</a>]
                [<a href="https://deepchem.io/tutorials/training-a-normalizing-flow-on-qm9/">3</a>].
                <br>
                I am developing a pipeline for arbitrary property optimization in Materials Science (along with Applied Materials!)
                <br>
            </p>
        </li>
        <!-- SAiDL -->
        <li>
            <p>
                <b>[RE] Teaching CLIP to Count to Ten</b>
                [<a href="https://arxiv.org/abs/2406.03586">arXiv</a> / <a href="https://github.com/SforAiDl/CountCLIP">GitHub</a>]
                <img src="images/saidl-logo.jpg" width="36" height="36">
                <br>
                Harshvardhan Mestha, Tejas Agrawal, Karan Bania, Shreyas V, Yash Bhisikar
            </p>
            <p>
            <details>
                <summary style="color: gray;">abstract</summary>
                <p style="width: 800px;">
                Large vision-language models (VLMs) are shown to learn rich joint image-text representations enabling high performances in relevant downstream tasks. 
                However, they fail to showcase their quantitative understanding of objects, and they lack good counting-aware representation. 
                This paper conducts a reproducibility study of 'Teaching CLIP to Count to Ten' (Paiss et al., 2023), which presents a method to finetune a CLIP model (Radford et al., 2021) to improve zero-shot counting accuracy in an image while maintaining the performance for zero-shot classification by introducing a counting-contrastive loss term. 
                We improve the model's performance on a smaller subset of their training data with lower computational resources. 
                We verify these claims by reproducing their study with our own code. 
                The implementation can be found at this <a href="https://github.com/SforAiDl/CountCLIP">https URL</a>.
                </p>
            </details>
            <br>
            This is a student-led reproduction of the Google Brain paper, <i>Teaching CLIP to Count to Ten</i> (<a href="https://arxiv.org/abs/2302.12066">arXiv</a>).
        </li>
    </ul>
    <br>


    <!-- 
        Acknowledgements
    -->
    <h3>Acknowledgement</h3>
    I am thoroughly grateful to all of my supervisors for their guidance and support.
    They have played a massive role in the way I think and approach problems.
    In no particular order, they are 
    <a href="https://www.bits-pilani.ac.in/goa/ashwin-srinivasan/">Ashwin Srinivasan</a> (Sr. Professor, BITS Goa), 
    <a href="https://bits-pilani.ac.in/goa/tanmay-tulsidas-verlekar/">Tanmay Verlekar</a> (Assistant Professor, BITS Goa), 
    <a href="https://researchers.mq.edu.au/en/persons/sidong-liu">Sidong Liu</a> (Full Professor, Macquarie University), 
    <a href="https://rbharath.github.io/about/">Bharath Ramsundar</a> (Founder, Deep Forest Sciences), 
    <a href="https://ekvv.uni-bielefeld.de/pers_publ/publ/PersonDetail.jsp?personId=503068013&lang=EN">David Kappel</a> (<s>PostDoc / Group Leader, RUB</s> Junior Professor, University of Bielefeld), 
    <a href="https://mlcv.inf.tu-dresden.de/group-schoene-mark.html">Mark Sch√∂ne</a> (PhD Student, TU Dresden), 
    and all of my fellow SAiDL members & seniors!
    <br><br>


    <!-- 
        Extra Stuff
    -->
    <details>
        <summary> Extra Stuff </summary>
        This is a list of random things, papers, tutorials, pictures, etc. that I find interesting.
        <ul>
            <li>
                One of the first PoPL papers I read, "<a href="https://dl.acm.org/doi/10.1145/3371106">A Simple Differentiable Programming Language</a>", it's quite dense!
                I'm probably still readung it xD. Thoroughly enjoyable.
            </li>
            <li>
                I am quite frequently asked by juniors on how to start learning machine-learning, and I recently took a small session on it,
                <a href="https://docs.google.com/presentation/d/1YfSpsF1VzESRTsmBJULyO1ndjawNZA62TKIo6HdblPU/edit?usp=sharing">slides</a>.
            </li>
            <li>
                My unique email count just hit 4! xD (a@gmail.com, b@goa.bits-pilani.ac.in, c@edu.ruhr-uni-bochum.de, d@deepforestsci.com)
            </li>
            <li>
                [The NIPS'14 Experiment] <a href="https://inverseprobability.com/talks/notes/the-neurips-experiment.html">Interesting</a> read.
            </li>
            <li>
                Great <a href="https://dmol.pub/index.html">book</a> for anyone wanting to enter the computational Chemistry / Materials Science field.
            </li>
            <li>
                Amazing <a href="https://srush.github.io/annotated-s4/">tutorial</a> on SSMs.
            </li>
            <li>
                This <a href="https://youtu.be/p8u_k2LIZyo?si=_JZbB7onplfniwQT">video</a> should be in CS curriculums!
            </li>
            <li>
                <a href="https://play.google.com/store/apps/details?id=com.pipsqueakgames.pkd&hl=en&gl=US&pli=1">Punch, Kick, Duck!</a>
                This took me a while but I am ranked <b>9th</b> in the world in this game's first level! xD.
            </li>
            <li>
                Craziest paper on GNNs so far! Ultra - <a href = "https://arxiv.org/pdf/2310.04562.pdf">Towards Foundation Models For Knowledge Graph Reasoning</a>.
            </li>
            <li>
                The most useful useless <a href="https://www.youtube.com/watch?v=rSKMYc1CQHE">video</a> I've seen.
            </li>
            <li>
                [‚ú®Zero Knowledge Proofs‚ú®] a (un)popular <a href="https://www.youtube.com/watch?v=fOGdb1CTu5c">video</a>.
            </li>
            <li>
                [üóºüóºüóº] <a href="images/eiffel.jpg">Your newest wallpaper</a> xD.
            </li>
            <li>
                [(Very) Amazing paper on GNNs!] GSAT - <a href = "https://arxiv.org/pdf/2201.12987.pdf">Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism</a>
                <br>
                My <a href="https://docs.google.com/presentation/d/1B9HIg9m8ZPFd3SBctBD1d50VCWpgNLteyG4FOP44EQo/edit?usp=sharing">Slides</a>.
            </li>
            <li>
                [Multi-task optimization üëåüèª] BLIP - <a href = "https://arxiv.org/pdf/2201.12086.pdf">Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>.
            </li>
            <li>
                [New paradigm on GNN's applicability] CrysXPP - <a href = "https://arxiv.org/pdf/2104.10869.pdf">An Explainable Property Predictor for Crystalline Materials</a>.
            </li>
            <li>
                [Google just being Google] Gshard - <a href = "https://arxiv.org/pdf/2006.16668.pdf">Scaling Giant Models with Conditional Computation and Automatic Sharding</a>
                <br> 
                My <a href="https://docs.google.com/presentation/d/1Nafdt7SyYD-4xiNwwqNemaRBDZrDZt4azRa_eKiQTow/edit?usp=sharing">Slides</a>.
            </li>
        </ul>
    </details>
    <br>
    <br>

</body>
